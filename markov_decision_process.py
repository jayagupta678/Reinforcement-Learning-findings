# -*- coding: utf-8 -*-
"""
Created on Thu Aug 29 12:11:19 2019

@author: E19SOE814
"""

import numpy as np


import numpy as np

def return_state_utility(v, T, u, reward, gamma):
    """Return the state utility.

    @param v the state vector
    @param T transition matrix
    @param u utility vector
    @param reward for that state
    @param gamma discount factor
    @return the utility of the state
    """
    action_array = np.zeros(4)
    for action in range(0, 4):
        action_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))
    return reward + gamma * np.max(action_array)

#def main():
#Starting state vector
#The agent starts from (1, 1)
v = np.array([[0.0, 0.0, 0.0, 0.0, 
               0.0, 0.0, 0.0, 0.0, 
               1.0, 0.0, 0.0, 0.0]])

#Transition matrix loaded from file
#(It is too big to write here)
T = np.load('E:\Jaya\Reinforcement Learning\Cleaning bot\T.npy')

#Utility vector
u = np.array([[0.812, 0.868, 0.918,   1.0,
               0.762,   0.0, 0.660,  -1.0,
               0.705, 0.655, 0.611, 0.388]])

#Defining the reward for state (1,1)
reward = -0.04
#Assuming that the discount factor is equal to 1.0
gamma = 1.0

#Use the Bellman equation to find the utility of state (1,1)
utility_11 = return_state_utility(v, T, u, reward, gamma)
print("Utility of state (1,1): " + str(utility_11))

#if __name__ == "__main__":
#    main()
